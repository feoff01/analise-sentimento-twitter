# -*- coding: utf-8 -*-
"""analise_twitter.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/147qxjETdl11DtYIux2jgQ_HlPo7Gvjy_
"""

!pip install pandas scikit-learn tensorflow keras

import pandas as pd
import numpy as np
import re
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, classification_report
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.utils import to_categorical

# Carregar os dados
train_data = pd.read_csv('twitter_training.csv', on_bad_lines='skip')
test_data = pd.read_csv('twitter_validation.csv')

# Visualizar a estrutura dos dados
print(train_data.head())
print(test_data.head())

train_data.columns = ['numero', 'palavra', 'sentiment','text']
test_data.columns = ['numero', 'palavra', 'sentiment','text']

# Remover colunas desnecessárias e verificar valores ausentes
train_data = train_data[['text', 'sentiment']]
train_data.dropna(inplace=True)

test_data = test_data[['text']]
test_data.dropna(inplace=True)

# Função de pré-processamento básico
def preprocess_text(text):
    # Conversão para minúsculas
    text = text.lower()
    # Remover pontuação e caracteres especiais
    text = re.sub(r'\W', ' ', text)
    # Remover números
    text = re.sub(r'\d', ' ', text)
    # Remover espaços extras
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# Aplicar pré-processamento
train_data['text'] = train_data['text'].apply(preprocess_text)
test_data['text'] = test_data['text'].apply(preprocess_text)

# Dividir os dados em conjuntos de treinamento e validação
X_train, X_val, y_train, y_val = train_test_split(
    train_data['text'], train_data['sentiment'], test_size=0.2, random_state=42
)

# Converter texto em vetores TF-IDF
vectorizer = TfidfVectorizer(max_features=5000)
X_train_tfidf = vectorizer.fit_transform(X_train).toarray()
X_val_tfidf = vectorizer.transform(X_val).toarray()
test_tfidf = vectorizer.transform(test_data['text']).toarray()

# Codificar os rótulos de saída
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_val_encoded = label_encoder.transform(y_val)

num_classes = len(np.unique(y_train_encoded))

y_train_categorical = to_categorical(y_train_encoded, num_classes=num_classes)
y_val_categorical = to_categorical(y_val_encoded, num_classes=num_classes)

# Construir o modelo
model = Sequential()
model.add(Dense(512, input_dim=5000, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(num_classes, activation='softmax'))  # número de classes

# Compilar o modelo
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Treinar o modelo
model.fit(X_train_tfidf, y_train_categorical, epochs=10, batch_size=64, validation_data=(X_val_tfidf, y_val_categorical))

# Previsões
y_pred = model.predict(X_val_tfidf)
y_pred_classes = np.argmax(y_pred, axis=1)
y_val_classes = np.argmax(y_val_categorical, axis=1)

# Avaliação
print('Acurácia:', accuracy_score(y_val_classes, y_pred_classes))
print('Relatório de Classificação:')
print(classification_report(y_val_classes, y_pred_classes))

# Previsões no conjunto de teste
test_predictions = model.predict(test_tfidf)
test_predictions_classes = np.argmax(test_predictions, axis=1)
test_predictions_labels = label_encoder.inverse_transform(test_predictions_classes)

# Salvar previsões
submission = pd.DataFrame({'id': test_data.index, 'sentiment': test_predictions_labels})
submission.to_csv('submission.csv', index=False)